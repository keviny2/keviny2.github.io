<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Tree-Based Methods</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="tree_based_methods.tex"> 
<link rel="stylesheet" type="text/css" href="tree_based_methods.css"> 
</head><body 
>
   <div class="maketitle">
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class="titleHead">Tree-Based Methods</h2>
<div class="author" ></div><br />
<div class="date" ><span 
class="cmr-12">July 20, 2022</span></div>
   </div>
<!--l. 19--><p class="indent" >   Tree-based methods are a great way to learn patterns in data. They are
extremely <span 
class="cmbx-10">interpretable</span>, making them a useful tool for communicating
findings in data science applications. Moreover, there are extensions to simpler
tree-based methods which are competitive in prediction accuracy. We will discuss
some of these extensions, but first, let us start with the basics through an
example.
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-10001"></a>Regression and Classification Trees</h3>
<!--l. 22--><p class="noindent" >Assume we are trying to predict the salary of a baseball player based on two
characteristics: the number of years they played in the major leagues, and
the number of hits they had last season. A possible tree that we might end
up with can be seen in Figure&#x00A0;<a 
href="#x1-1001r1">1<!--tex4ht:ref: regtree --></a>. There are two places where the tree is
split: Years<span 
class="cmmi-10">&#x003C;</span>4.5 and Hits<span 
class="cmmi-10">&#x003C;</span>117.5. These are the tree&#8217;s <span 
class="cmti-10">internal nodes</span>, and
we call the two split conditions Years<span 
class="cmmi-10">&#x003C;</span>4.5 and Hits<span 
class="cmmi-10">&#x003C;</span>117.5 the &#8217;splitting
rules&#8217;.
<!--l. 24--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-1001r1"></a>
                                                                  

                                                                  
<!--l. 26--><p class="noindent" ><img 
src="regression_tree.png" alt="PIC"  
width="36" height="36" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1: </span><span  
class="content">Example of a regression tree to predict salary from years played and
number of hits from last season</span></div><!--tex4ht:label?: x1-1001r1 -->
                                                                  

                                                                  
<!--l. 29--><p class="indent" >   </div><hr class="endfigure">
<!--l. 31--><p class="indent" >   How exactly do we make a prediction given a new observation? Consider a player
named Bob who has played 6 years in the major leagues and got 96 hits last
season. According to the tree, the first split checks whether the number of
years this player has played in the major leagues is less than 4.5. If it is less
than 4.5, we travel to the left, otherwise, to the right (you can swap the
right/left directions as long as it is consistent throughout the tree). Since Bob
has played greater than 4.5 years, we travel to the right. Now we arrive
at another &#8217;splitting rule&#8217; which asks if the number of hits this player got
from last season is less than 117.5. Bob had 96 hits last season; thus, we
travel to the left and get to the <span 
class="cmti-10">terminal or leaf node </span>from which we make
our prediction. In this case, we predict Bob to make <span 
class="cmmi-10">e</span><sup><span 
class="cmr-7">6</span></sup> <span 
class="cmsy-10">* </span>1000 <span 
class="cmsy-10">&#x2248; </span>403<span 
class="cmmi-10">,</span>429
dollars.
<!--l. 33--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-1002r2"></a>
                                                                  

                                                                  
<!--l. 35--><p class="noindent" ><img 
src="predictor_space.png" alt="PIC"  
width="32" height="32" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;2:  </span><span  
class="content">Partitioned  predictor  space  based  on  splitting  rules  depicted  in
Figure&#x00A0;<a 
href="#x1-1001r1">1<!--tex4ht:ref: regtree --></a></span></div><!--tex4ht:label?: x1-1002r2 -->
                                                                  

                                                                  
<!--l. 38--><p class="indent" >   </div><hr class="endfigure">
<!--l. 40--><p class="indent" >   Currently, we only discussed how to interpret a tree. Moreover, we have
not talked about the differences between regression and classification trees.
In order to elucidate what differentiates the two, we turn our attention to
how a tree partitions the predictor space. Examining Figure&#x00A0;<a 
href="#x1-1002r2">2<!--tex4ht:ref: predspace --></a>, we see three
regions annotated by <span 
class="cmmi-10">R</span>1<span 
class="cmmi-10">,R</span>2<span 
class="cmmi-10">,R</span>3 and observations within each denoted by
<span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">R</span><sub><span 
class="cmmi-5">i</span></sub></sub> = <span 
class="cmsy-10">{</span><span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> : <span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> <span 
class="cmsy-10">&#x2208; </span><span 
class="cmmi-10">R</span><sub><span 
class="cmmi-7">i</span></sub><span 
class="cmsy-10">}</span>. Each region corresponds to a terminal node in the tree.
Herein lies the key difference between regression and classification trees: for
regression trees, we take a quantitative statistic such as the mean of all data
points within the region, whereas for classification trees, we take a statistic
over qualitative data such as the mode. In contrast, for classification trees,
we take a statistic for qualitative data such as the mode. For instance, a
possible statistic for <span 
class="cmmi-10">R</span>1 could be the mean of all observations that lie in <span 
class="cmmi-10">R</span>1,
<img 
src="tree_based_methods0x.png" alt="-1
n"  class="frac" align="middle"> <span 
class="cmex-10">&#x2211;</span>
    <sub><span 
class="cmmi-7">i</span><span 
class="cmr-7">=1</span></sub><sup><span 
class="cmmi-7">n</span></sup>.
   <h4 class="subsectionHead"><span class="titlemark">1.1   </span> <a 
 id="x1-20001.1"></a>Pruning</h4>
<!--l. 43--><p class="noindent" >An important question to ask when building a tree is when to stop. Theoretically, the
resulting tree can have a terminal node for every single data point in the
entire training set and obtain 100% training accuracy; in this case, we will
almost certainly be overfitting to the training set. Pruning is a technique
that mitigates this problem by decreasing the number of terminal nodes by
removing branches. Naturally, we should remove branches that do not increase
the error rate by much. One method that accomplishes this is called <span 
class="cmti-10">cost</span>
<span 
class="cmti-10">complexity pruning</span>. Cost complexity pruning assumes an augmented loss
function,
<!--l. 45--><p class="indent" >
<table 
class="align-star">
                     <tr><td 
class="align-odd"><span 
class="cmex-10">&#x2211;</span>
  <sub><span 
class="cmmi-7">m</span><span 
class="cmr-7">=1</span></sub><sup><span 
class="cmsy-7">|</span><span 
class="cmmi-7">T</span><span 
class="cmsy-7">|</span></sup><span 
class="cmex-10">&#x2211;</span>
   <sub><span 
class="cmmi-7">x</span><sub><span 
class="cmmi-5">i</span></sub><span 
class="cmsy-7">&#x2208;</span><span 
class="cmmi-7">R</span><sub><span 
class="cmmi-5">m</span></sub></sub>(<span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> <span 
class="cmsy-10">-</span><span 
class="cmmi-10">&#x0177;</span><sub><span 
class="cmmi-7">R</span><sub><span 
class="cmmi-5">m</span></sub></sub>)<sup><span 
class="cmr-7">2</span></sup> + <span 
class="cmmi-10">&#x03B1;</span><span 
class="cmsy-10">|</span><span 
class="cmmi-10">T</span><span 
class="cmsy-10">|</span></td>                     <td 
class="align-even"></td>                     <td 
class="align-label"></td></tr></table>
                                                                  

                                                                  
<!--l. 48--><p class="indent" >   where <span 
class="cmsy-10">|</span><span 
class="cmmi-10">T</span><span 
class="cmsy-10">| </span>is the number of terminal nodes in the tree, <span 
class="cmmi-10">R</span><sub><span 
class="cmmi-7">m</span></sub> is the region indexed
by <span 
class="cmmi-10">m</span>, <span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> are the training observations, <span 
class="cmmi-10">&#x0177;</span><sub><span 
class="cmmi-7">R</span><sub><span 
class="cmmi-5">m</span></sub></sub> is the estimate for region <span 
class="cmmi-10">R</span><sub><span 
class="cmmi-7">m</span></sub> and <span 
class="cmmi-10">&#x03B1; </span>is a
non-negative tuning parameter. The additional <span 
class="cmmi-10">&#x03B1;</span><span 
class="cmsy-10">|</span><span 
class="cmmi-10">T</span><span 
class="cmsy-10">| </span>term penalizes trees with many
terminal nodes, meaning very deep trees will have a higher loss, making the
algorithm prune branches. Since <span 
class="cmmi-10">&#x03B1; </span>is not fixed, it is typical to learn it using cross
validation. A grid search is performed to identify values of <span 
class="cmmi-10">&#x03B1; </span>which result in trees
with differing numbers of terminal nodes. Then, cross validation can be performed on
each individual tree to determine the optimal tree. See Figure&#x00A0;<a 
href="#x1-2001r3">3<!--tex4ht:ref: pruning --></a> for an example of
pruning.
<!--l. 50--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-2001r3"></a>
                                                                  

                                                                  
<!--l. 52--><p class="noindent" ><img 
src="pruning.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3:  </span><span  
class="content">Top:  unpruned  classification  tree;  Bottom  Left:  Cross  validation
results for cost complexity pruning; Bottom Right: Pruned classification tree</span></div><!--tex4ht:label?: x1-2001r3 -->
                                                                  

                                                                  
<!--l. 56--><p class="indent" >   </div><hr class="endfigure">
   <h4 class="subsectionHead"><span class="titlemark">1.2   </span> <a 
 id="x1-30001.2"></a>Pros vs. Cons of Trees</h4>
<!--l. 60--><p class="noindent" >As mentioned in the beginning, trees can be easily interpreted. The split rules
are simple to understand, and can reveal connections between individual
predictors and the outcome. Furthermore, the way a tree makes predictions
is thought to mirror human decision-making better than other methods.
Trees can also be displayed graphically, making it a desirable option for
presentations in real-world applications. Despite these advantages, trees suffer
from lower prediction accuracy, high variance, and lack of robustness (a
small change in the data leads to a big change in the resulting tree). In
view of this, we shift our focus to some extensions to trees that tackle these
limitations.
                                                                  

                                                                  
<!--l. 64--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-40002"></a>Extensions to Tree-methods</h3>
<!--l. 65--><p class="noindent" >We discuss some extensions to traditional tree-methods. A graph comparing the
accuracy of Boosting and Random Forests can be seen in Figure&#x00A0;<a 
href="#x1-4001r4">4<!--tex4ht:ref: accuracy --></a>
<!--l. 67--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-4001r4"></a>
                                                                  

                                                                  
<!--l. 69--><p class="noindent" ><img 
src="tree_extension.png" alt="PIC"  
width="43" height="43" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;4: </span><span  
class="content">Test classification error rates for Boosting and Random Forest</span></div><!--tex4ht:label?: x1-4001r4 -->
                                                                  

                                                                  
<!--l. 72--><p class="indent" >   </div><hr class="endfigure">
   <h4 class="subsectionHead"><span class="titlemark">2.1   </span> <a 
 id="x1-50002.1"></a>Bagging</h4>
<!--l. 75--><p class="noindent" >As mentioned above, tree-methods suffer from high variance. In statistics, we know
that averaging a set of observations reduces variance. Bagging leverages this fact by
generating &#8216;additional&#8217; datasets using bootstrapping and fitting an unpruned tree to
each one (choosing not to prune is fine because we are reducing variance by fitting
multiple trees). Therefore, if we use bootstrapping to obtain <span 
class="cmmi-10">B </span>training datasets and
define <img 
src="tree_based_methods1x.png" alt="&#x02C6;f"  class="circ" > <sup><span 
class="cmsy-7">*</span><span 
class="cmmi-7">b</span></sup>(<span 
class="cmmi-10">x</span>) to be the tree fitted to dataset <span 
class="cmmi-10">b </span><span 
class="cmsy-10">&#x2208; </span><span 
class="cmmi-10">B</span>, then our bagging estimate for
a test case <span 
class="cmmi-10">x </span>will be the average over all estimates from each individual
tree,
<table 
class="align-star">
                       <tr><td 
class="align-odd"><img 
src="tree_based_methods2x.png" alt="&#x02C6;f"  class="circ" > <sub><span 
class="cmmi-7">bag</span></sub>(<span 
class="cmmi-10">x</span>) = <img 
src="tree_based_methods3x.png" alt="-1
B"  class="frac" align="middle"><span 
class="cmex-10">&#x2211;</span>
    <sub><span 
class="cmmi-7">b</span><span 
class="cmr-7">=1</span></sub><sup><span 
class="cmmi-7">B</span></sup><img 
src="tree_based_methods4x.png" alt="&#x02C6;f"  class="circ" > <sup><span 
class="cmsy-7">*</span><span 
class="cmmi-7">b</span></sup>(<span 
class="cmmi-10">x</span>)</td>                       <td 
class="align-even"></td>                       <td 
class="align-label"></td></tr></table>
<!--l. 80--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2   </span> <a 
 id="x1-60002.2"></a>Random Forests</h4>
<!--l. 81--><p class="noindent" >A drawback of Bagging lies in the fact that each of the <span 
class="cmmi-10">B </span>trees are correlated. This
is due to the fact that it is often the case that certain predictors are very
effective, so many splits across trees will use the same predictor. This ultimately
makes trees not completely independent, detracting from the goal of reducing
variance. Random Forests decorrelates the trees by only considering a random
fraction of predictors at each split (typically <img 
src="tree_based_methods5x.png" alt="&#x221A;p"  class="sqrt" > where <span 
class="cmmi-10">p </span>is the total number of
predictors.
                                                                  

                                                                  
<!--l. 83--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.3   </span> <a 
 id="x1-70002.3"></a>Boosting</h4>
<!--l. 85--><p class="noindent" >Boosting takes on a different flavor than Bagging and Random Forests. Rather than
fit multiple trees on bootstrapped datasets, Boosting instead learns &#8216;slowly&#8217; in an
additive fashion. Specifically, given a current model, we fit a decision tree to the
residuals. This way, each individual tree can be said to tackle a different
part of the problem. Since each tree is not targeting the same thing, they
are not correlated. Boosting uses hyperparameters <span 
class="cmmi-10">&#x03BB;</span>, which controls the
amount of contribution each individual tree has in the final prediction, and an
integer value denoting the depth for each decision tree. See Algorithm 1 for a
description.
   <div class="algorithm">
                                                                  

                                                                  
<!--l. 88--><p class="indent" >   <a 
 id="x1-7001r1"></a><hr class="float"><div class="float" 
>
                                                                  

                                                                  
 <div class="caption" 
><span class="id">Algorithm 1: </span><span  
class="content">Hello</span></div><!--tex4ht:label?: x1-7001r1 -->
<div class="algorithmic">
       <span class="label-11.99998pt">
 <span 
class="cmr-8">1:</span> </span>&#xA0;<span 
class="algorithmic">&#x00A0;<span 
class="cmbx-10">procedure</span>&#x00A0;<span 
class="cmcsc-10">B<span 
class="small-caps">o</span><span 
class="small-caps">o</span><span 
class="small-caps">s</span><span 
class="small-caps">t</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span><span 
class="small-caps">g</span></span>(<span 
class="cmmi-10">x</span>)        </span><br class="algorithmic"/><span class="label-11.99998pt">  <span 
class="cmr-8">2:</span> </span>&#xA0;<span  class="algorithmic">&#x00A0;    Set <img  src="tree_based_methods6x.png" alt="&#x02C6;f"  class="circ" > (<span 
class="cmmi-10">x</span>) = 0 and <span 
class="cmmi-10">r</span><sub><span 
class="cmmi-7">i</span></sub> = <span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> for all <span 
class="cmmi-10">i </span>in the training set        </span><br class="algorithmic"/><span class="label-11.99998pt">  <span 
class="cmr-8">3:</span> </span>&#xA0;<span  class="algorithmic">&#x00A0;    <span 
class="cmbx-10">for</span>&#x00A0;<span 
class="cmmi-10">b </span>= 1<span 
class="cmmi-10">,</span>2<span 
class="cmmi-10">,</span><span 
class="cmmi-10">&#x2026;</span><span 
class="cmmi-10">,B</span>&#x00A0;<span 
class="cmbx-10">do</span>        </span><br class="algorithmic"/><span class="label-11.99998pt">  <span 
class="cmr-8">4:</span> </span>&#xA0;<span  class="algorithmic">&#x00A0;         Fit a tree <img  src="tree_based_methods7x.png" alt="&#x02C6;f"  class="circ" > <sup><span 
class="cmmi-7">b</span></sup> with <span 
class="cmmi-10">d </span>splits (<span 
class="cmmi-10">d</span>+1 terminal nodes) to the trining data    (<span 
class="cmmi-10">X,r</span>)        </span><br class="algorithmic"/><span class="label-11.99998pt">  <span 
class="cmr-8">5:</span> </span>&#xA0;<span  class="algorithmic">&#x00A0;         Update <img  src="tree_based_methods8x.png" alt="&#x02C6;f"  class="circ" > by adding in a shrunken version of the new tree:    <table  class="align-star">                                    <tr><td  class="align-odd"><img  src="tree_based_methods9x.png" alt="&#x02C6;f"  class="circ" > (<span 
class="cmmi-10">x</span>) <span 
class="cmsy-10">&#x2190;</span><img  src="tree_based_methods10x.png" alt="f&#x02C6;"  class="circ" > (<span 
class="cmmi-10">x</span>) + <span 
class="cmmi-10">&#x03BB;</span><img  src="tree_based_methods11x.png" alt="f&#x02C6;"  class="circ" > <sup><span 
class="cmmi-7">b</span></sup>(<span 
class="cmmi-10">x</span>)</td>                                   <td  class="align-even"></td>                                   <td  class="align-label"></td></tr></table>        </span><br class="algorithmic"/><span class="label-11.99998pt">  <span 
class="cmr-8">6:</span> </span>&#xA0;<span  class="algorithmic">&#x00A0;         Update the residuals,                                                                                                                                           <table  class="align-star">                                       <tr><td  class="align-odd"><span 
class="cmmi-10">r</span><sub><span 
class="cmmi-7">i</span></sub> <span 
class="cmsy-10">&#x2190; </span><span 
class="cmmi-10">r</span><sub><span 
class="cmmi-7">i</span></sub> <span 
class="cmsy-10">- </span><span 
class="cmmi-10">&#x03BB;</span><img  src="tree_based_methods12x.png" alt="&#x02C6;f"  class="circ" > <sup><span 
class="cmmi-7">b</span></sup>(<span 
class="cmmi-10">x</span><sub> <span 
class="cmmi-7">i</span></sub>)</td>                                      <td  class="align-even"></td>                                      <td  class="align-label"></td></tr></table>        </span><br class="algorithmic"/><span class="label-11.99998pt">    </span>&#xA0;<span  class="algorithmic">&#x00A0;-         <span 
class="cmmi-10">Outputtheboostedmodel,</span>     </span><br class="algorithmic"/><span class="label-11.99998pt"> <span 
class="cmr-8">7:</span><span 
class="cmr-8">8:</span></span>&#xA0;<span  class="algorithmic">&#x00A0;</span>
</div>
                                                                  

                                                                  
   </div><hr class="endfloat" />
   </div>
   <h4 class="subsectionHead"><span class="titlemark">2.4   </span> <a 
 id="x1-80002.4"></a>Regression Trees</h4>
     <ul class="itemize1">
     <li class="itemize">
     <!--l. 114--><p class="noindent" >consists of a series of splitting rules
     </li>
     <li class="itemize">
     <!--l. 115--><p class="noindent" ><span 
class="cmbx-10">terminology</span>
         <ul class="itemize2">
         <li class="itemize">
         <!--l. 117--><p class="noindent" ><span 
class="cmti-10">terminal node/leaf </span>: resulting regions due to splits in predictor space
         </li>
         <li class="itemize">
         <!--l. 118--><p class="noindent" ><span 
class="cmti-10">internal node</span>: points at which predictor space is split
         </li>
         <li class="itemize">
         <!--l. 119--><p class="noindent" ><span 
class="cmti-10">branch</span>: segments of tree that connect nodes</li></ul>
     </li>
     <li class="itemize">
     <!--l. 121--><p class="noindent" >easy to interpret
     </li>
     <li class="itemize">
     <!--l. 122--><p class="noindent" >goal is to find boxes <span 
class="cmmi-10">R</span><sub><span 
class="cmr-7">1</span></sub><span 
class="cmmi-10">,</span><span 
class="cmmi-10">&#x2026;</span><span 
class="cmmi-10">,R</span><sub><span 
class="cmmi-7">J</span></sub> that minimizes the RSS (residual sum of
     squares)
     </li>
     <li class="itemize">
     <!--l. 123--><p class="noindent" ><span 
class="cmti-10">pruning: </span>grow a very large tree then prune it back to obtain a subtree
         <ul class="itemize2">
         <li class="itemize">
         <!--l. 125--><p class="noindent" ><span 
class="cmti-10">cost complexity pruning (weakest link pruning)</span>: generate a sequence of
         successively smaller subtrees by increasing a nonnegative tuning parameter
         <span 
class="cmmi-10">&#x03B1; </span>until pruning lowers the loss function,
                                                                  

                                                                  
         <table 
class="align-star">
                          <tr><td 
class="align-odd"><span 
class="cmex-10">&#x2211;</span>
  <sub><span 
class="cmmi-7">m</span><span 
class="cmr-7">=1</span></sub><sup><span 
class="cmsy-7">|</span><span 
class="cmmi-7">T</span><span 
class="cmsy-7">|</span></sup><span 
class="cmex-10">&#x2211;</span>
   <sub><span 
class="cmmi-7">x</span><sub><span 
class="cmmi-5">i</span></sub><span 
class="cmsy-7">&#x2208;</span><span 
class="cmmi-7">R</span><sub><span 
class="cmmi-5">m</span></sub></sub>(<span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> <span 
class="cmsy-10">-</span><span 
class="cmmi-10">&#x0177;</span><sub><span 
class="cmmi-7">R</span><sub><span 
class="cmmi-5">m</span></sub></sub>)<sup><span 
class="cmr-7">2</span></sup> + <span 
class="cmmi-10">&#x03B1;</span><span 
class="cmsy-10">|</span><span 
class="cmmi-10">T</span><span 
class="cmsy-10">|</span></td>           <td 
class="align-even"></td>           <td 
class="align-label"></td></tr></table>
         <!--l. 129--><p class="noindent" >Each <span 
class="cmmi-10">&#x03B1; </span>corresponds to a different subtree. We learn which subtree and <span 
class="cmmi-10">&#x03B1; </span>is
         optimal through CV</li></ul>
     </li></ul>
<!--l. 133--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.5   </span> <a 
 id="x1-90002.5"></a>Classification Trees</h4>
     <ul class="itemize1">
     <li class="itemize">
     <!--l. 135--><p class="noindent" >same as regression tree, but different loss functions
         <ul class="itemize2">
         <li class="itemize">
         <!--l. 137--><p class="noindent" >Gini index
         <table 
class="align-star">
                              <tr><td 
class="align-odd"><span 
class="cmmi-10">G </span>= <span 
class="cmex-10">&#x2211;</span>
  <sub><span 
class="cmmi-7">k</span><span 
class="cmr-7">=1</span></sub><sup><span 
class="cmmi-7">K</span></sup><img 
src="tree_based_methods13x.png" alt="&#x02C6;p"  class="circ" > <sub>
<span 
class="cmmi-7">mk</span></sub>(1 <span 
class="cmsy-10">-</span><img 
src="tree_based_methods14x.png" alt="&#x02C6;p"  class="circ" > <sub><span 
class="cmmi-7">mk</span></sub>)</td>                              <td 
class="align-even"></td>                              <td 
class="align-label"></td></tr></table>
                                                                  

                                                                  
         </li>
         <li class="itemize">
         <!--l. 141--><p class="noindent" >Cross-entropy
         <table 
class="align-star">
                              <tr><td 
class="align-odd"><span 
class="cmmi-10">D </span>= <span 
class="cmsy-10">-</span><span 
class="cmex-10">&#x2211;</span>
  <sub><span 
class="cmmi-7">k</span><span 
class="cmr-7">=1</span></sub><sup><span 
class="cmmi-7">K</span></sup><img 
src="tree_based_methods15x.png" alt="&#x02C6;p"  class="circ" > <sub>
<span 
class="cmmi-7">mk</span></sub> log <img 
src="tree_based_methods16x.png" alt="&#x02C6;p"  class="circ" > <sub><span 
class="cmmi-7">mk</span></sub></td>                               <td 
class="align-even"></td>                               <td 
class="align-label"></td></tr></table>
         </li>
         <li class="itemize">
         <!--l. 145--><p class="noindent" >both Gini index or cross-entropy are typically used for tree-growing
         because they measure node <span 
class="cmti-10">purity</span></li></ul>
     </li>
     <li class="itemize">
     <!--l. 147--><p class="noindent" >sometimes a split does not reduce classification error (answer is the same
     for either split), but it does improve the node purity (Gini index and
     cross-entropy)</li></ul>
<!--l. 150--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.6   </span> <a 
 id="x1-100002.6"></a>Pros vs. Cons of Trees</h4>
     <ul class="itemize1">
     <li class="itemize">
     <!--l. 152--><p class="noindent" ><span 
class="cmbx-10">Pros:</span>
         <ul class="itemize2">
         <li class="itemize">
         <!--l. 154--><p class="noindent" >Interpretable
         </li>
         <li class="itemize">
         <!--l. 155--><p class="noindent" >Mirrors human decision-making better
                                                                  

                                                                  
         </li>
         <li class="itemize">
         <!--l. 156--><p class="noindent" >Can be displayed graphically
         </li>
         <li class="itemize">
         <!--l. 157--><p class="noindent" >Easily   handle   qualitative   predictors   without   creating   dummy
         variables</li></ul>
     </li>
     <li class="itemize">
     <!--l. 159--><p class="noindent" ><span 
class="cmbx-10">Cons:</span>
         <ul class="itemize2">
         <li class="itemize">
         <!--l. 161--><p class="noindent" >Lower prediction accuracy
         </li>
         <li class="itemize">
         <!--l. 162--><p class="noindent" >High variance
         </li>
         <li class="itemize">
         <!--l. 163--><p class="noindent" >Non-robust;  small  change  in  data  leads  to  big  change  in  final
         estimated tree</li></ul>
     </li></ul>
<!--l. 167--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.7   </span> <a 
 id="x1-110002.7"></a>Bagging</h4>
     <ul class="itemize1">
     <li class="itemize">
     <!--l. 169--><p class="noindent" >Use bootstrapping to obtain <span 
class="cmmi-10">B </span>training datasets, train for each dataset to
     obtain <img 
src="tree_based_methods17x.png" alt="&#x02C6;f"  class="circ" > <sup><span 
class="cmsy-7">*</span><span 
class="cmmi-7">b</span></sup>(<span 
class="cmmi-10">x</span>) and average the predictions:
                                                                  

                                                                  
     <table 
class="align-star">
                                 <tr><td 
class="align-odd"><img 
src="tree_based_methods18x.png" alt="&#x02C6;f"  class="circ" > <sub><span 
class="cmmi-7">bag</span></sub>(<span 
class="cmmi-10">x</span>) = <img 
src="tree_based_methods19x.png" alt="1
--
B"  class="frac" align="middle"><span 
class="cmex-10">&#x2211;</span>
    <sub><span 
class="cmmi-7">b</span><span 
class="cmr-7">=1</span></sub><sup><span 
class="cmmi-7">B</span></sup><img 
src="tree_based_methods20x.png" alt="&#x02C6;f"  class="circ" > <sup><span 
class="cmsy-7">*</span><span 
class="cmmi-7">b</span></sup>(<span 
class="cmmi-10">x</span>)</td>                                 <td 
class="align-even"></td>                                 <td 
class="align-label"></td></tr></table>
     </li>
     <li class="itemize">
     <!--l. 173--><p class="noindent" >Trees are not pruned
     </li>
     <li class="itemize">
     <!--l. 174--><p class="noindent" ><span 
class="cmbx-10">Out-of-bag Error Estimation: </span>for each observation <span 
class="cmmi-10">i</span>, only use trees where
     this observation was not used (out-of-bag) in training to make the prediction
     and compute test error
     </li>
     <li class="itemize">
     <!--l. 175--><p class="noindent" >Hard to interpret; can compute variable importance measures (the total
     amount that the RSS decreased due to splits over a certain predictor, averaged
     over all <span 
class="cmmi-10">B </span>trees)</li></ul>
<!--l. 178--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.8   </span> <a 
 id="x1-120002.8"></a>Random Forests</h4>
     <ul class="itemize1">
     <li class="itemize">
     <!--l. 180--><p class="noindent" >Only consider a random fraction of predictors at each split (typically <img 
src="tree_based_methods21x.png" alt="&#x221A;-
 p"  class="sqrt" >,
     where <span 
class="cmmi-10">p </span>is the total number of predictors)
     </li>
     <li class="itemize">
     <!--l. 181--><p class="noindent" >Decorrelates trees, making the average less variable compared to Bagging</li></ul>
<!--l. 184--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.9   </span> <a 
 id="x1-130002.9"></a>Boosting</h4>
                                                                  

                                                                  
     <ul class="itemize1">
     <li class="itemize">
     <!--l. 186--><p class="noindent" >Learn slowly: given the current model, fit a decision tree to the residuals.
     Repeat in an additive manner.
     </li>
     <li class="itemize">
     <!--l. 187--><p class="noindent" >Construction of each tree depends strongly on trees that are already grown</li></ul>
    
</body></html> 

                                                                  


